# Neural Networks: Zero to Hero 🚀

This repository contains my implementations and notes from Andrej Karpathy's **Neural Networks: Zero to Hero** series. The course provides a hands-on, code-first approach to deep learning, guiding learners from the fundamentals of neural networks to building complex models like GPT.

---

## 📚 Course Syllabus

1. **Micrograd**: Implementing a minimalistic neural network engine to understand backpropagation and automatic differentiation.
2. **Makemore (Bigram Model)**: Building a character-level language model using bigrams to predict the next character.
3. **Makemore (MLP)**: Enhancing the language model with a multilayer perceptron to improve performance.
4. **Makemore (BatchNorm & Activations)**: Introducing batch normalization and exploring different activation functions.
5. **Makemore (Backpropagation)**: Manually implementing backpropagation through the network to deepen understanding.
6. **Makemore (WaveNet)**: Constructing a WaveNet-style architecture for sequence modeling.
7. **GPT**: Building a Generative Pretrained Transformer from scratch, following the architecture of GPT-2 and GPT-3.
8. **Tokenizer**: Developing a tokenizer to convert text into tokens suitable for language models.

---

## 🛠️ Technologies Used

- Python 3
- PyTorch
- NumPy
- Google Colab

---

## 🎯 Objective

The goal of this project is to gain a deep understanding of neural networks by implementing them from scratch, without relying on high-level libraries, and to build up to creating complex models like GPT.

---

## 📺 Course Link

You can access the full course here:  
[Neural Networks: Zero to Hero](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)

---

## 🔗 Related Repositories

- [micrograd](https://github.com/karpathy/micrograd)
- [makemore](https://github.com/karpathy/makemore)
- [nanoGPT](https://github.com/karpathy/nanoGPT)
